{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All revv'ed up and ready to go!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import cv2                \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt    \n",
    "import random\n",
    "\n",
    "#from extract_bottleneck_features import *\n",
    "from glob import glob\n",
    "\n",
    "from keras.applications.resnet50 import decode_predictions, preprocess_input, ResNet50\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint  \n",
    "from keras.layers import Conv2D, Dense, Dropout, Flatten, GlobalAveragePooling2D, MaxPooling2D\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import image                  \n",
    "\n",
    "from sklearn.datasets import load_files       \n",
    "\n",
    "from PIL import ImageFile\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_Xception(tensor):\n",
    "    from keras.applications.xception import Xception, preprocess_input\n",
    "    return Xception(weights='imagenet', include_top=False).predict(preprocess_input(tensor))\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"All revv'ed up and ready to go!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to load train, test, and validation datasets\n",
    "NUM_CATEGORIES = 3\n",
    "\n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    files = np.array(data['filenames'])\n",
    "    targets = np_utils.to_categorical(np.array(data['target']), NUM_CATEGORIES)\n",
    "    return files, targets\n",
    "\n",
    "# load train, test, and validation datasets\n",
    "base_folder = 'data'\n",
    "valid_files, valid_targets = load_dataset('{}/valid'.format(base_folder))\n",
    "\n",
    "# load list of disease names\n",
    "disease_names = [item.split(\"/\")[2] for item in sorted(glob(\"{}/valid/*/\".format(base_folder)))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['melanoma', 'nevus', 'seborrheic_keratosis']\n",
      "Examples of file ['data/valid/melanoma/ISIC_0013644.jpg' 'data/valid/nevus/ISIC_0015443.jpg'\n",
      " 'data/valid/nevus/ISIC_0012313.jpg'\n",
      " 'data/valid/seborrheic_keratosis/ISIC_0012720.jpg'\n",
      " 'data/valid/nevus/ISIC_0007332.jpg'] and target [[ 1.  0.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]]\n",
      "There are 3 total disease categories.\n",
      "There are 150 validation disease images.\n"
     ]
    }
   ],
   "source": [
    "#for item in sorted(glob(\"{}/valid/*/\".format(base_folder))):\n",
    "#    print()\n",
    "\n",
    "print(disease_names)\n",
    "\n",
    "print('Examples of file {} and target {}'.format(valid_files[-5:], valid_targets[-5:]))\n",
    "\n",
    "#print(valid_files[-5:])\n",
    "#print(valid_targets[-5:])\n",
    "\n",
    "print('There are %d total disease categories.' % len(disease_names))\n",
    "print('There are %d validation disease images.' % len(valid_files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2750 total disease images.\n",
      "\n",
      "There are 2000 training disease images.\n",
      "There are 600 test disease images.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# These are separated as I am using my computer to setup the framework, and AWS for training\n",
    "# To save space, I didn't download train/test, a total of 10GB as zip!\n",
    "\n",
    "train_files, train_targets = load_dataset('{}/train'.format(base_folder))\n",
    "test_files, test_targets = load_dataset('{}/test'.format(base_folder))\n",
    "\n",
    "print('There are %s total disease images.\\n' % len(np.hstack([train_files, valid_files, test_files])))\n",
    "print('There are %d training disease images.' % len(train_files))\n",
    "print('There are %d test disease images.'% len(test_files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [04:32<00:00,  7.33it/s]\n",
      "100%|██████████| 150/150 [00:36<00:00,  4.68it/s]\n",
      "100%|██████████| 600/600 [03:47<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done, be sure to run this just once (per Jupyter notebook session)!\n"
     ]
    }
   ],
   "source": [
    "                           \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True                 \n",
    "\n",
    "# pre-process the data for Keras\n",
    "train_tensors = paths_to_tensor(train_files).astype('float32')/255\n",
    "valid_tensors = paths_to_tensor(valid_files).astype('float32')/255\n",
    "test_tensors = paths_to_tensor(test_files).astype('float32')/255\n",
    "\n",
    "print(\"All done, be sure to run this just once (per Jupyter notebook session)!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting valid tensors\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## No beuno\n",
    "#bottleneck_features = np.load('bottleneck_features/DogXceptionData.npz')\n",
    "#train_Xception = bottleneck_features['train']\n",
    "#valid_Xception = bottleneck_features['valid']\n",
    "#test_Xception = bottleneck_features['test']\n",
    "\n",
    "print('Converting valid tensors')\n",
    "valid_Xception = np.array([extract_Xception(np.expand_dims(cur_tensor, axis=0)) for cur_tensor in valid_tensors])\n",
    "print('Converting train tensors')\n",
    "train_Xception = np.array([extract_Xception(np.expand_dims(cur_tensor, axis=0)) for cur_tensor in train_tensors])\n",
    "print('Converting test tensors')\n",
    "test_Xception = np.array([extract_Xception(np.expand_dims(cur_tensor, axis=0)) for cur_tensor in test_tensors])\n",
    "\n",
    "\n",
    "#for cur_tensor in valid_tensors:\n",
    "#    print(cur_tensor[0][0][0])\n",
    "    #valid_Xception.append()\n",
    "\n",
    "#valid_Xception = np.array(valid_Xception)\n",
    "\n",
    "print('ALLL LDOOOOOONE>...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "global_average_pooling2d_1 ( (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 6147      \n",
      "=================================================================\n",
      "Total params: 6,147.0\n",
      "Trainable params: 6,147.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "Xception_model = Sequential()\n",
    "Xception_model.add(GlobalAveragePooling2D(input_shape = train_Xception.shape[1:]))\n",
    "Xception_model.add(Dense(len(disease_names), activation = 'softmax'))\n",
    "\n",
    "Xception_model.summary()\n",
    "\n",
    "Xception_model.compile(loss = 'categorical_crossentropy'\n",
    "                        , metrics = ['accuracy']\n",
    "                        , optimizer = 'rmsprop')\n",
    "\n",
    "Xception_model.compile(loss = 'categorical_crossentropy'\n",
    "                        , metrics = ['accuracy']\n",
    "                        , optimizer = 'rmsprop')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 7, 2048)\n",
      "(7, 7, 2048)\n",
      "(3,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "835\n",
      "150\n",
      "6680\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "print(train_Xception.shape[1:])\n",
    "print(valid_Xception.shape[1:])\n",
    "print(valid_targets.shape[1:])\n",
    "\n",
    "print(type(train_Xception))\n",
    "print(type(np.array([])))\n",
    "\n",
    "print(len(valid_Xception))\n",
    "print(len(valid_targets))\n",
    "\n",
    "print(len(train_Xception))\n",
    "print(len(train_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input arrays should have the same number of samples as target arrays. Found 6680 input samples and 2000 target samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-62eee32a48bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m                    \u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                    \u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvalid_Xception\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                    \u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m                    )\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    843\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 845\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    846\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1403\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1404\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1405\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1406\u001b[0m         \u001b[0;31m# prepare validation data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1407\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1305\u001b[0m                           \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m                           in zip(y, sample_weights, class_weights, self._feed_sample_weight_modes)]\n\u001b[0;32m-> 1307\u001b[0;31m         \u001b[0m_check_array_lengths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1308\u001b[0m         _check_loss_and_target_compatibility(y,\n\u001b[1;32m   1309\u001b[0m                                              \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_loss_fns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_check_array_lengths\u001b[0;34m(inputs, targets, weights)\u001b[0m\n\u001b[1;32m    227\u001b[0m                          \u001b[0;34m'the same number of samples as target arrays. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m                          \u001b[0;34m'Found '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' input samples '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m                          'and ' + str(list(set_y)[0]) + ' target samples.')\n\u001b[0m\u001b[1;32m    230\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mset_y\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mset_w\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         raise ValueError('Sample_weight arrays should have '\n",
      "\u001b[0;31mValueError\u001b[0m: Input arrays should have the same number of samples as target arrays. Found 6680 input samples and 2000 target samples."
     ]
    }
   ],
   "source": [
    "\n",
    "best_Xception_model_file = 'saved_models/weights.best.Xception.hdf5'\n",
    "    \n",
    "checkpointer = ModelCheckpoint(filepath = best_Xception_model_file\n",
    "                               , save_best_only = True\n",
    "                               , verbose = 1)\n",
    "\n",
    "Xception_model.fit(train_Xception, train_targets \n",
    "                   , batch_size = 20\n",
    "                   , callbacks = [checkpointer]\n",
    "                   , epochs = 20\n",
    "                   , validation_data = (valid_Xception, valid_targets)\n",
    "                   , verbose = 1\n",
    "                   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "Xception_model.load_weights(best_Xception_model_file)\n",
    "\n",
    "# get index of predicted dog breed for each image in test set\n",
    "Xception_predictions = [np.argmax(Xception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Xception]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(Xception_predictions)==np.argmax(test_targets, axis=1))/len(Xception_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def disease(image_path):\n",
    "    img_tensor = path_to_tensor(image_path)\n",
    "    feature = extract_Xception(img_tensor)\n",
    "    #print(feature)\n",
    "    #print(feature.shape)\n",
    "    index = np.argmax(Xception_model.predict(feature)) #np.expand_dims(feature, axis=0)))\n",
    "    \n",
    "    return disease_names[index]\n",
    "\n",
    "disease('./data/valid/melanoma/ISIC_0012099.jpg')\n",
    "disease('./data/valid/nevus/ISIC_0001769.jpg')\n",
    "disease('./data/valid/seborrheic_keratosis/ISIC_0012143.jpg')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
