{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All revv'ed up and ready to go!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import cv2                \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt    \n",
    "import random\n",
    "\n",
    "#from extract_bottleneck_features import *\n",
    "from glob import glob\n",
    "\n",
    "from keras.applications.resnet50 import decode_predictions, preprocess_input, ResNet50\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint  \n",
    "from keras.layers import Conv2D, Dense, Dropout, Flatten, GlobalAveragePooling2D, MaxPooling2D\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import image                  \n",
    "\n",
    "from sklearn.datasets import load_files       \n",
    "\n",
    "from PIL import ImageFile\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_Xception(tensor):\n",
    "    from keras.applications.xception import Xception, preprocess_input\n",
    "    return Xception(weights='imagenet', include_top=False).predict(preprocess_input(tensor))\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"All revv'ed up and ready to go!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to load train, test, and validation datasets\n",
    "NUM_CATEGORIES = 3\n",
    "\n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    files = np.array(data['filenames'])\n",
    "    targets = np_utils.to_categorical(np.array(data['target']), NUM_CATEGORIES)\n",
    "    return files, targets\n",
    "\n",
    "# load train, test, and validation datasets\n",
    "base_folder = 'data'\n",
    "valid_files, valid_targets = load_dataset('{}/valid'.format(base_folder))\n",
    "\n",
    "# load list of disease names\n",
    "disease_names = [item.split(\"/\")[2] for item in sorted(glob(\"{}/valid/*/\".format(base_folder)))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['melanoma', 'nevus', 'seborrheic_keratosis']\n",
      "Examples of file ['data/valid/melanoma/ISIC_0013644.jpg' 'data/valid/nevus/ISIC_0015443.jpg'\n",
      " 'data/valid/nevus/ISIC_0012313.jpg'\n",
      " 'data/valid/seborrheic_keratosis/ISIC_0012720.jpg'\n",
      " 'data/valid/nevus/ISIC_0007332.jpg'] and target [[ 1.  0.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]]\n",
      "There are 3 total disease categories.\n",
      "There are 150 validation disease images.\n"
     ]
    }
   ],
   "source": [
    "#for item in sorted(glob(\"{}/valid/*/\".format(base_folder))):\n",
    "#    print()\n",
    "\n",
    "print(disease_names)\n",
    "\n",
    "print('Examples of file {} and target {}'.format(valid_files[-5:], valid_targets[-5:]))\n",
    "\n",
    "#print(valid_files[-5:])\n",
    "#print(valid_targets[-5:])\n",
    "\n",
    "print('There are %d total disease categories.' % len(disease_names))\n",
    "print('There are %d validation disease images.' % len(valid_files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# These are separated as I am using my computer to setup the framework, and AWS for training\n",
    "# To save space, I didn't download train/test, a total of 10GB as zip!\n",
    "\n",
    "train_files, train_targets = load_dataset('{}/train'.format(base_folder))\n",
    "test_files, test_targets = load_dataset('{}/test'.format(base_folder))\n",
    "\n",
    "print('There are %s total disease images.\\n' % len(np.hstack([train_files, valid_files, test_files])))\n",
    "print('There are %d training disease images.' % len(train_files))\n",
    "print('There are %d test disease images.'% len(test_files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "                           \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True                 \n",
    "\n",
    "# pre-process the data for Keras\n",
    "train_tensors = paths_to_tensor(train_files).astype('float32')/255\n",
    "valid_tensors = paths_to_tensor(valid_files).astype('float32')/255\n",
    "test_tensors = paths_to_tensor(test_files).astype('float32')/255\n",
    "\n",
    "print(\"All done, be sure to run this just once (per Jupyter notebook session)!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "bottleneck_features = np.load('bottleneck_features/DogXceptionData.npz')\n",
    "\n",
    "train_Exception = bottleneck_features['train']\n",
    "valid_Exception = bottleneck_features['valid']\n",
    "test_Exception = bottleneck_features['test']\n",
    "\n",
    "\n",
    "Exception_model = Sequential()\n",
    "Exception_model.add(GlobalAveragePooling2D(input_shape = train_Exception.shape[1:]))\n",
    "Exception_model.add(Dense(len(dog_names), activation = 'softmax'))\n",
    "\n",
    "Exception_model.summary()\n",
    "\n",
    "Exception_model.compile(loss = 'categorical_crossentropy'\n",
    "                        , metrics = ['accuracy']\n",
    "                        , optimizer = 'rmsprop')\n",
    "\n",
    "Exception_model.compile(loss = 'categorical_crossentropy'\n",
    "                        , metrics = ['accuracy']\n",
    "                        , optimizer = 'rmsprop')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "best_exception_model_file = 'saved_models/weights.best.Exception.hdf5'\n",
    "    \n",
    "checkpointer = ModelCheckpoint(filepath = best_inception_model_file\n",
    "                               , save_best_only = True\n",
    "                               , verbose = 1)\n",
    "\n",
    "Exception_model.fit(train_InceptionV3, train_targets \n",
    "                    , batch_size = 20\n",
    "                    , callbacks = [checkpointer]\n",
    "                    , epochs = 20\n",
    "                    , validation_data = (valid_Exception, valid_targets)\n",
    "                    , verbose = 1\n",
    "                   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "Exception_model.load_weights(best_inception_model_file)\n",
    "\n",
    "# get index of predicted dog breed for each image in test set\n",
    "Exception_predictions = [np.argmax(Exception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Exception]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(Exception_predictions)==np.argmax(test_targets, axis=1))/len(Exception_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def disease(image_path):\n",
    "    img_tensor = path_to_tensor(image_path)\n",
    "    feature = extract_InceptionV3(img_tensor)\n",
    "    #print(feature)\n",
    "    #print(feature.shape)\n",
    "    doggo_index = np.argmax(InceptionV3_model.predict(feature)) #np.expand_dims(feature, axis=0)))\n",
    "    \n",
    "    return dog_names[doggo_index]\n",
    "\n",
    "disease('./data/valid/melanoma/ISIC_0012099.jpg')\n",
    "disease('./data/valid/nevus/ISIC_0001769.jpg')\n",
    "disease('./data/valid/seborrheic_keratosis/ISIC_0012143.jpg')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
