{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All revv'ed up and ready to go!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import cv2                \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt    \n",
    "import random\n",
    "\n",
    "#from extract_bottleneck_features import *\n",
    "from glob import glob\n",
    "\n",
    "from keras.applications.resnet50 import decode_predictions, preprocess_input, ResNet50\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Conv2D, Dense, Dropout, Flatten, GlobalAveragePooling2D, MaxPooling2D, PReLU\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import image                  \n",
    "\n",
    "from sklearn.datasets import load_files       \n",
    "\n",
    "from PIL import ImageFile\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_Xception(tensor):\n",
    "    from keras.applications.xception import Xception, preprocess_input\n",
    "    return Xception(weights='imagenet', include_top=False).predict(preprocess_input(tensor))\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"All revv'ed up and ready to go!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to load train, test, and validation datasets\n",
    "NUM_CATEGORIES = 3\n",
    "\n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    files = np.array(data['filenames'])\n",
    "    targets = np_utils.to_categorical(np.array(data['target']), NUM_CATEGORIES)\n",
    "    return files, targets\n",
    "\n",
    "# load train, test, and validation datasets\n",
    "base_folder = 'data'\n",
    "valid_files, valid_targets = load_dataset('{}/valid'.format(base_folder))\n",
    "\n",
    "# load list of disease names\n",
    "disease_names = [item.split(\"/\")[2] for item in sorted(glob(\"{}/valid/*/\".format(base_folder)))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['melanoma', 'nevus', 'seborrheic_keratosis']\n",
      "Examples of file ['data/valid/melanoma/ISIC_0013644.jpg' 'data/valid/nevus/ISIC_0015443.jpg'\n",
      " 'data/valid/nevus/ISIC_0012313.jpg'\n",
      " 'data/valid/seborrheic_keratosis/ISIC_0012720.jpg'\n",
      " 'data/valid/nevus/ISIC_0007332.jpg'] and target [[ 1.  0.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]]\n",
      "There are 3 total disease categories.\n",
      "There are 150 validation disease images.\n"
     ]
    }
   ],
   "source": [
    "#for item in sorted(glob(\"{}/valid/*/\".format(base_folder))):\n",
    "#    print()\n",
    "\n",
    "print(disease_names)\n",
    "\n",
    "print('Examples of file {} and target {}'.format(valid_files[-5:], valid_targets[-5:]))\n",
    "\n",
    "#print(valid_files[-5:])\n",
    "#print(valid_targets[-5:])\n",
    "\n",
    "print('There are %d total disease categories.' % len(disease_names))\n",
    "print('There are %d validation disease images.' % len(valid_files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2000 training disease images.\n",
      "There are 600 test disease images.\n",
      "There are 2750 total disease images.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# These are separated as I am using my computer to setup the framework, and AWS for training\n",
    "# To save space, I didn't download train/test, a total of 10GB as zip!\n",
    "\n",
    "train_files, train_targets = load_dataset('{}/train'.format(base_folder))\n",
    "test_files, test_targets = load_dataset('{}/test'.format(base_folder))\n",
    "\n",
    "print('There are %d training disease images.' % len(train_files))\n",
    "print('There are %d test disease images.'% len(test_files))\n",
    "\n",
    "print('There are %s total disease images.\\n' % len(np.hstack([train_files, valid_files, test_files])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [04:32<00:00,  7.33it/s]\n",
      "100%|██████████| 150/150 [00:36<00:00,  4.69it/s]\n",
      "100%|██████████| 600/600 [03:47<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done, be sure to run this just once (per Jupyter notebook session)!\n"
     ]
    }
   ],
   "source": [
    "                           \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True                 \n",
    "\n",
    "# pre-process the data for Keras\n",
    "train_tensors = paths_to_tensor(train_files).astype('float32')/255\n",
    "valid_tensors = paths_to_tensor(valid_files).astype('float32')/255\n",
    "test_tensors = paths_to_tensor(test_files).astype('float32')/255\n",
    "\n",
    "print(\"All done, be sure to run this just once (per Jupyter notebook session)!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 223, 223, 16)      208       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 111, 111, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 110, 110, 32)      2080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 55, 55, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 54, 54, 64)        8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 27, 27, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 27, 27, 64)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_2 ( (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 10,739.0\n",
      "Trainable params: 10,739.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "activation_func = 'relu' # PReLU\n",
    "# TODO: Prelu is better than relu!  Per a single paper I read...  FIXME: How to use here? ^^\n",
    "\n",
    "padding_type = 'valid' # Use valid padding for now to match suggested model size per above\n",
    "output_activation_func = 'softmax'\n",
    "kernel_and_pool_size = 2\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(\n",
    "    activation = activation_func\n",
    "    , filters = 16\n",
    "    , input_shape = train_tensors[0].shape\n",
    "    , kernel_size = kernel_and_pool_size\n",
    "    , padding = padding_type\n",
    "))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size = kernel_and_pool_size))\n",
    "\n",
    "model.add(Conv2D(\n",
    "    activation = activation_func\n",
    "    , filters = 32\n",
    "    , kernel_size = kernel_and_pool_size\n",
    "    , padding = padding_type\n",
    "))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size = kernel_and_pool_size))\n",
    "\n",
    "model.add(Conv2D(\n",
    "    activation= activation_func\n",
    "    , filters = 64\n",
    "    , kernel_size = kernel_and_pool_size\n",
    "    , padding = padding_type\n",
    "))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size = kernel_and_pool_size))\n",
    "\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(GlobalAveragePooling2D())\n",
    "\n",
    "#model.add(Flatten()) # GlobalAveragePooling does a form of flattenning...\n",
    "#model.add(Dense(500, activation = activation_func)) # cifar10_cnn has this, leaving out for now\n",
    "#model.add(Dropout(0.4))                             # in order to follow original suggestion\n",
    "\n",
    "model.add(Dense(len(disease_names), activation = output_activation_func))\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 150 samples\n",
      "Epoch 1/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.8025 - acc: 0.6848Epoch 00000: val_loss improved from inf to 0.96618, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "2000/2000 [==============================] - 6s - loss: 0.8015 - acc: 0.6860 - val_loss: 0.9662 - val_acc: 0.5200\n",
      "Epoch 2/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.8020 - acc: 0.6854Epoch 00001: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.8008 - acc: 0.6860 - val_loss: 1.0218 - val_acc: 0.5200\n",
      "Epoch 3/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.8004 - acc: 0.6854Epoch 00002: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7989 - acc: 0.6860 - val_loss: 1.0118 - val_acc: 0.5200\n",
      "Epoch 4/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7934 - acc: 0.6874Epoch 00003: val_loss improved from 0.96618 to 0.92262, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "2000/2000 [==============================] - 6s - loss: 0.7951 - acc: 0.6860 - val_loss: 0.9226 - val_acc: 0.5200\n",
      "Epoch 5/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7939 - acc: 0.6848Epoch 00004: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7934 - acc: 0.6855 - val_loss: 0.9310 - val_acc: 0.5200\n",
      "Epoch 6/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7918 - acc: 0.6869Epoch 00005: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7918 - acc: 0.6870 - val_loss: 0.9742 - val_acc: 0.5200\n",
      "Epoch 7/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7894 - acc: 0.6848Epoch 00006: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7894 - acc: 0.6850 - val_loss: 0.9433 - val_acc: 0.5200\n",
      "Epoch 8/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7879 - acc: 0.6854Epoch 00007: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7872 - acc: 0.6855 - val_loss: 0.9578 - val_acc: 0.5200\n",
      "Epoch 9/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7869 - acc: 0.6869Epoch 00008: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7868 - acc: 0.6865 - val_loss: 0.9265 - val_acc: 0.5200\n",
      "Epoch 10/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7813 - acc: 0.6859Epoch 00009: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7806 - acc: 0.6855 - val_loss: 0.9692 - val_acc: 0.5200\n",
      "Epoch 11/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7840 - acc: 0.6869Epoch 00010: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7823 - acc: 0.6875 - val_loss: 1.0123 - val_acc: 0.5200\n",
      "Epoch 12/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7775 - acc: 0.6879Epoch 00011: val_loss improved from 0.92262 to 0.91066, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "2000/2000 [==============================] - 6s - loss: 0.7784 - acc: 0.6865 - val_loss: 0.9107 - val_acc: 0.5133\n",
      "Epoch 13/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7764 - acc: 0.6843Epoch 00012: val_loss improved from 0.91066 to 0.89092, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "2000/2000 [==============================] - 6s - loss: 0.7771 - acc: 0.6835 - val_loss: 0.8909 - val_acc: 0.5200\n",
      "Epoch 14/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7811 - acc: 0.6828Epoch 00013: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7782 - acc: 0.6840 - val_loss: 1.0936 - val_acc: 0.5200\n",
      "Epoch 15/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7738 - acc: 0.6884Epoch 00014: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7738 - acc: 0.6880 - val_loss: 0.9444 - val_acc: 0.5133\n",
      "Epoch 16/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7756 - acc: 0.6859Epoch 00015: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7745 - acc: 0.6865 - val_loss: 0.9992 - val_acc: 0.5133\n",
      "Epoch 17/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7736 - acc: 0.6854Epoch 00016: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7724 - acc: 0.6855 - val_loss: 0.9114 - val_acc: 0.5133\n",
      "Epoch 18/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7732 - acc: 0.6838Epoch 00017: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7709 - acc: 0.6865 - val_loss: 0.9606 - val_acc: 0.5133\n",
      "Epoch 19/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7649 - acc: 0.6843Epoch 00018: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7684 - acc: 0.6825 - val_loss: 0.9222 - val_acc: 0.5133\n",
      "Epoch 20/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7730 - acc: 0.6838Epoch 00019: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7701 - acc: 0.6855 - val_loss: 1.0103 - val_acc: 0.5200\n",
      "Epoch 21/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7692 - acc: 0.6879Epoch 00020: val_loss improved from 0.89092 to 0.86087, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "2000/2000 [==============================] - 6s - loss: 0.7694 - acc: 0.6870 - val_loss: 0.8609 - val_acc: 0.5400\n",
      "Epoch 22/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7673 - acc: 0.6848Epoch 00021: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7657 - acc: 0.6855 - val_loss: 0.9288 - val_acc: 0.5133\n",
      "Epoch 23/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7626 - acc: 0.6838Epoch 00022: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7629 - acc: 0.6835 - val_loss: 0.9203 - val_acc: 0.5133\n",
      "Epoch 24/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7660 - acc: 0.6833Epoch 00023: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7633 - acc: 0.6845 - val_loss: 1.0988 - val_acc: 0.5200\n",
      "Epoch 25/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7654 - acc: 0.6854Epoch 00024: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7667 - acc: 0.6845 - val_loss: 0.9405 - val_acc: 0.5133\n",
      "Epoch 26/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7624 - acc: 0.6828Epoch 00025: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7612 - acc: 0.6835 - val_loss: 0.9543 - val_acc: 0.5133\n",
      "Epoch 27/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7612 - acc: 0.6848Epoch 00026: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7615 - acc: 0.6850 - val_loss: 0.8954 - val_acc: 0.5067\n",
      "Epoch 28/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7625 - acc: 0.6848Epoch 00027: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7635 - acc: 0.6845 - val_loss: 0.9017 - val_acc: 0.5133\n",
      "Epoch 29/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7624 - acc: 0.6884Epoch 00028: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7614 - acc: 0.6885 - val_loss: 0.9404 - val_acc: 0.5067\n",
      "Epoch 30/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7619 - acc: 0.6848Epoch 00029: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7623 - acc: 0.6845 - val_loss: 0.9067 - val_acc: 0.5133\n",
      "Epoch 31/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7607 - acc: 0.6879Epoch 00030: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7605 - acc: 0.6880 - val_loss: 1.0514 - val_acc: 0.5200\n",
      "Epoch 32/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7711 - acc: 0.6859Epoch 00031: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 6s - loss: 0.7700 - acc: 0.6865 - val_loss: 0.9203 - val_acc: 0.5067\n",
      "Epoch 33/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7562 - acc: 0.6899Epoch 00032: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7566 - acc: 0.6895 - val_loss: 0.9727 - val_acc: 0.5200\n",
      "Epoch 34/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7617 - acc: 0.6864Epoch 00033: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7631 - acc: 0.6865 - val_loss: 0.9102 - val_acc: 0.5133\n",
      "Epoch 35/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7598 - acc: 0.6879Epoch 00034: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7603 - acc: 0.6870 - val_loss: 0.9059 - val_acc: 0.5067\n",
      "Epoch 36/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7574 - acc: 0.6854Epoch 00035: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7571 - acc: 0.6850 - val_loss: 0.9218 - val_acc: 0.5133\n",
      "Epoch 37/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7570 - acc: 0.6889Epoch 00036: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7564 - acc: 0.6905 - val_loss: 0.9366 - val_acc: 0.5133\n",
      "Epoch 38/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7563 - acc: 0.6869Epoch 00037: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7547 - acc: 0.6880 - val_loss: 1.0510 - val_acc: 0.5200\n",
      "Epoch 39/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7557 - acc: 0.6874Epoch 00038: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7554 - acc: 0.6870 - val_loss: 0.9416 - val_acc: 0.5067\n",
      "Epoch 40/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7588 - acc: 0.6833Epoch 00039: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7572 - acc: 0.6845 - val_loss: 1.1778 - val_acc: 0.5200\n",
      "Epoch 41/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7579 - acc: 0.6874Epoch 00040: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7580 - acc: 0.6875 - val_loss: 0.9068 - val_acc: 0.5200\n",
      "Epoch 42/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7569 - acc: 0.6838Epoch 00041: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7554 - acc: 0.6840 - val_loss: 0.9620 - val_acc: 0.5133\n",
      "Epoch 43/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7540 - acc: 0.6833Epoch 00042: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7546 - acc: 0.6830 - val_loss: 0.8899 - val_acc: 0.5067\n",
      "Epoch 44/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7564 - acc: 0.6848Epoch 00043: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7598 - acc: 0.6830 - val_loss: 0.9093 - val_acc: 0.5067\n",
      "Epoch 45/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7593 - acc: 0.6828Epoch 00044: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7587 - acc: 0.6835 - val_loss: 0.8945 - val_acc: 0.5067\n",
      "Epoch 46/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7471 - acc: 0.6909Epoch 00045: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7459 - acc: 0.6920 - val_loss: 0.9321 - val_acc: 0.5133\n",
      "Epoch 47/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7521 - acc: 0.6854Epoch 00046: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7500 - acc: 0.6865 - val_loss: 0.9995 - val_acc: 0.5200\n",
      "Epoch 48/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7511 - acc: 0.6899Epoch 00047: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7497 - acc: 0.6910 - val_loss: 0.8989 - val_acc: 0.5733\n",
      "Epoch 49/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7571 - acc: 0.6894Epoch 00048: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7575 - acc: 0.6890 - val_loss: 0.8977 - val_acc: 0.5133\n",
      "Epoch 50/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7525 - acc: 0.6889Epoch 00049: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7521 - acc: 0.6885 - val_loss: 0.9051 - val_acc: 0.5133\n",
      "Epoch 51/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7516 - acc: 0.6909Epoch 00050: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7495 - acc: 0.6920 - val_loss: 0.8949 - val_acc: 0.5133\n",
      "Epoch 52/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7494 - acc: 0.6833Epoch 00051: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7473 - acc: 0.6840 - val_loss: 1.0116 - val_acc: 0.5133\n",
      "Epoch 53/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7532 - acc: 0.6929Epoch 00052: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7534 - acc: 0.6925 - val_loss: 0.9141 - val_acc: 0.5200\n",
      "Epoch 54/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7496 - acc: 0.6939Epoch 00053: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7521 - acc: 0.6915 - val_loss: 0.8875 - val_acc: 0.5000\n",
      "Epoch 55/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7521 - acc: 0.6838Epoch 00054: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7506 - acc: 0.6855 - val_loss: 0.9994 - val_acc: 0.5133\n",
      "Epoch 56/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7585 - acc: 0.6848Epoch 00055: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7567 - acc: 0.6860 - val_loss: 0.9274 - val_acc: 0.5133\n",
      "Epoch 57/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7478 - acc: 0.6859Epoch 00056: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7483 - acc: 0.6850 - val_loss: 0.9031 - val_acc: 0.5133\n",
      "Epoch 58/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7430 - acc: 0.6854Epoch 00057: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7426 - acc: 0.6855 - val_loss: 0.8947 - val_acc: 0.5200\n",
      "Epoch 59/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7494 - acc: 0.6899Epoch 00058: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7504 - acc: 0.6885 - val_loss: 0.8696 - val_acc: 0.5267\n",
      "Epoch 60/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7532 - acc: 0.6869Epoch 00059: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7529 - acc: 0.6870 - val_loss: 0.8888 - val_acc: 0.5200\n",
      "Epoch 61/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7482 - acc: 0.6874Epoch 00060: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7477 - acc: 0.6880 - val_loss: 0.8847 - val_acc: 0.5333\n",
      "Epoch 62/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7491 - acc: 0.6899Epoch 00061: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7510 - acc: 0.6890 - val_loss: 0.8771 - val_acc: 0.5133\n",
      "Epoch 63/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7450 - acc: 0.6914Epoch 00062: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7465 - acc: 0.6905 - val_loss: 0.8662 - val_acc: 0.5267\n",
      "Epoch 64/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7516 - acc: 0.6874Epoch 00063: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7494 - acc: 0.6890 - val_loss: 0.9090 - val_acc: 0.5133\n",
      "Epoch 65/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7451 - acc: 0.6929Epoch 00064: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 6s - loss: 0.7444 - acc: 0.6930 - val_loss: 0.8836 - val_acc: 0.5267\n",
      "Epoch 66/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7473 - acc: 0.6939Epoch 00065: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7460 - acc: 0.6945 - val_loss: 0.9043 - val_acc: 0.5200\n",
      "Epoch 67/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7468 - acc: 0.6884Epoch 00066: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7491 - acc: 0.6880 - val_loss: 0.8766 - val_acc: 0.5267\n",
      "Epoch 68/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7501 - acc: 0.6874Epoch 00067: val_loss improved from 0.86087 to 0.85457, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "2000/2000 [==============================] - 6s - loss: 0.7511 - acc: 0.6865 - val_loss: 0.8546 - val_acc: 0.5333\n",
      "Epoch 69/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7494 - acc: 0.6909Epoch 00068: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7482 - acc: 0.6910 - val_loss: 0.9124 - val_acc: 0.5267\n",
      "Epoch 70/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7509 - acc: 0.6879Epoch 00069: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7480 - acc: 0.6900 - val_loss: 1.0134 - val_acc: 0.5267\n",
      "Epoch 71/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7488 - acc: 0.6864Epoch 00070: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7479 - acc: 0.6870 - val_loss: 0.8779 - val_acc: 0.5267\n",
      "Epoch 72/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7426 - acc: 0.6894Epoch 00071: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7434 - acc: 0.6875 - val_loss: 0.8611 - val_acc: 0.5333\n",
      "Epoch 73/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7424 - acc: 0.6919Epoch 00072: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7424 - acc: 0.6920 - val_loss: 0.9073 - val_acc: 0.5133\n",
      "Epoch 74/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7439 - acc: 0.6838Epoch 00073: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7427 - acc: 0.6855 - val_loss: 0.9207 - val_acc: 0.5200\n",
      "Epoch 75/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7378 - acc: 0.6889Epoch 00074: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7407 - acc: 0.6880 - val_loss: 0.8860 - val_acc: 0.5400\n",
      "Epoch 76/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7447 - acc: 0.6899Epoch 00075: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7436 - acc: 0.6900 - val_loss: 0.9056 - val_acc: 0.5200\n",
      "Epoch 77/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7414 - acc: 0.6939Epoch 00076: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7428 - acc: 0.6930 - val_loss: 0.8622 - val_acc: 0.5333\n",
      "Epoch 78/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7379 - acc: 0.6874Epoch 00077: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7396 - acc: 0.6865 - val_loss: 0.9080 - val_acc: 0.5267\n",
      "Epoch 79/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7381 - acc: 0.6924Epoch 00078: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7373 - acc: 0.6930 - val_loss: 0.9266 - val_acc: 0.5267\n",
      "Epoch 80/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7365 - acc: 0.6960Epoch 00079: val_loss improved from 0.85457 to 0.85348, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "2000/2000 [==============================] - 6s - loss: 0.7394 - acc: 0.6960 - val_loss: 0.8535 - val_acc: 0.5333\n",
      "Epoch 81/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7333 - acc: 0.6990Epoch 00080: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7343 - acc: 0.6980 - val_loss: 0.9435 - val_acc: 0.5133\n",
      "Epoch 82/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7364 - acc: 0.6960Epoch 00081: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7390 - acc: 0.6945 - val_loss: 0.8549 - val_acc: 0.5400\n",
      "Epoch 83/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7489 - acc: 0.6894Epoch 00082: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7475 - acc: 0.6895 - val_loss: 0.8946 - val_acc: 0.5333\n",
      "Epoch 84/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7340 - acc: 0.6924Epoch 00083: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7343 - acc: 0.6915 - val_loss: 0.8764 - val_acc: 0.5333\n",
      "Epoch 85/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7352 - acc: 0.6919Epoch 00084: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7342 - acc: 0.6930 - val_loss: 0.8759 - val_acc: 0.5400\n",
      "Epoch 86/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7305 - acc: 0.7010Epoch 00085: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7323 - acc: 0.6995 - val_loss: 0.8599 - val_acc: 0.5467\n",
      "Epoch 87/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7360 - acc: 0.6975Epoch 00086: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7377 - acc: 0.6975 - val_loss: 0.8931 - val_acc: 0.5267\n",
      "Epoch 88/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7353 - acc: 0.6970Epoch 00087: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7375 - acc: 0.6955 - val_loss: 0.8861 - val_acc: 0.5533\n",
      "Epoch 89/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7340 - acc: 0.6944Epoch 00088: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7333 - acc: 0.6945 - val_loss: 0.9242 - val_acc: 0.5267\n",
      "Epoch 90/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7403 - acc: 0.6939Epoch 00089: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7392 - acc: 0.6955 - val_loss: 0.8774 - val_acc: 0.5400\n",
      "Epoch 91/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7323 - acc: 0.6919Epoch 00090: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7327 - acc: 0.6920 - val_loss: 0.8714 - val_acc: 0.5333\n",
      "Epoch 92/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7335 - acc: 0.6889Epoch 00091: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7328 - acc: 0.6885 - val_loss: 0.8938 - val_acc: 0.5333\n",
      "Epoch 93/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7304 - acc: 0.6934Epoch 00092: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7282 - acc: 0.6950 - val_loss: 0.8664 - val_acc: 0.5533\n",
      "Epoch 94/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7304 - acc: 0.6965Epoch 00093: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7311 - acc: 0.6960 - val_loss: 0.8625 - val_acc: 0.5533\n",
      "Epoch 95/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7298 - acc: 0.6955Epoch 00094: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7316 - acc: 0.6950 - val_loss: 0.8842 - val_acc: 0.5400\n",
      "Epoch 96/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7379 - acc: 0.6899Epoch 00095: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7341 - acc: 0.6925 - val_loss: 0.9107 - val_acc: 0.5200\n",
      "Epoch 97/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7301 - acc: 0.6955Epoch 00096: val_loss improved from 0.85348 to 0.83107, saving model to saved_models/weights.best.from_scratch.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 6s - loss: 0.7306 - acc: 0.6950 - val_loss: 0.8311 - val_acc: 0.5600\n",
      "Epoch 98/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7279 - acc: 0.6985Epoch 00097: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7273 - acc: 0.6990 - val_loss: 0.9044 - val_acc: 0.5400\n",
      "Epoch 99/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7319 - acc: 0.6949Epoch 00098: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7281 - acc: 0.6975 - val_loss: 0.9160 - val_acc: 0.5333\n",
      "Epoch 100/100\n",
      "1980/2000 [============================>.] - ETA: 0s - loss: 0.7325 - acc: 0.6960Epoch 00099: val_loss did not improve\n",
      "2000/2000 [==============================] - 6s - loss: 0.7336 - acc: 0.6945 - val_loss: 0.8410 - val_acc: 0.5400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0e81a8ce80>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "epochs = 100\n",
    "\n",
    "checkpointer = ModelCheckpoint(\n",
    "    filepath = 'saved_models/weights.best.from_scratch.hdf5'\n",
    "    , save_best_only = True\n",
    "    , verbose = 1\n",
    ")\n",
    "\n",
    "model.fit(train_tensors, train_targets\n",
    "          , batch_size = 20\n",
    "          , callbacks = [checkpointer]\n",
    "          , epochs = epochs\n",
    "          , validation_data = (valid_tensors, valid_targets)\n",
    "          , verbose = 1\n",
    "         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 64.0000%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.load_weights('saved_models/weights.best.from_scratch.hdf5')\n",
    "\n",
    "# get index of predicted dog breed for each image in test set\n",
    "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nevus nevus nevus\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def disease(image_path):\n",
    "    img_tensor = path_to_tensor(image_path)\n",
    "    #feature = extract_Xception(img_tensor)\n",
    "    #print(feature)\n",
    "    #print(feature.shape)\n",
    "    index = np.argmax(model.predict(img_tensor)) #np.expand_dims(feature, axis=0)))\n",
    "    \n",
    "    return disease_names[index]\n",
    "\n",
    "print(disease('./data/valid/melanoma/ISIC_0012099.jpg')\n",
    "      , disease('./data/valid/nevus/ISIC_0001769.jpg')\n",
    "      ,disease('./data/valid/seborrheic_keratosis/ISIC_0012143.jpg'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
